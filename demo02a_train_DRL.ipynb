{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "from DrlLibs.training import train_drl_agent\n",
    "from DrlLibs.evaluate import evaluate_drl_agent\n",
    "from DrlLibs import create_environment, check_env\n",
    "from Configs import getEnvConfig, visualizeEnvConfig, getPredictorConfig, visualizePredictorConfig\n",
    "from EnvLibs import PolicyDemoAdaptiveAlpha, PolicySimulator, createEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(simParams, simEnv, save_path, agent_name, algorithm_name: str = \"SAC\", \n",
    "         obvMode=\"perfect\",\n",
    "         total_timesteps: int = 20000, timesteps_per_episode: int = 5000):\n",
    "    \"\"\"Main function to train and evaluate a DRL agent.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"{algorithm_name} as Agent config{agent_name}'s Training and Evaluation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create environment\n",
    "    print(\"Creating environment...\")\n",
    "    env = create_environment(simParams, simEnv, obvMode, timesteps_per_episode)\n",
    "    \n",
    "    # Check environment\n",
    "    print(\"Checking environment...\")\n",
    "    check_env(env.unwrapped)\n",
    "    print(\"Environment check passed!\")\n",
    "    \n",
    "    # Train DRL agent\n",
    "    model, callback, training_time = train_drl_agent(algorithm_name, env, total_timesteps, save_path, agent_name)\n",
    "    \n",
    "    # Evaluate DRL agent\n",
    "    eval_results = evaluate_drl_agent(model, env, algorithm_name)\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Algorithm: {algorithm_name}\")\n",
    "    print(f\"Training completed in: {training_time:.2f} seconds\")\n",
    "    print(f\"Total training timesteps: {total_timesteps}\")\n",
    "    print(f\"Average evaluation reward: {eval_results['avg_reward']:.4f} ± {eval_results['std_reward']:.4f}\")\n",
    "    print(f\"Average packet loss rate: {eval_results['avg_loss_rate']:.4f} ± {eval_results['std_loss_rate']:.4f}\")\n",
    "    print(f\"Average alpha value: {eval_results['avg_alpha']:.4f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return model, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Environment Configuration\n",
      "==================================================\n",
      "Number of Users:        4\n",
      "Window Length:          200\n",
      "Dataflow:               thumb_fr\n",
      "N_aggregation:          4\n",
      "Resource Bar:           5\n",
      "Bandwidth:              100\n",
      "M List:                 [2, 3, 4]\n",
      "Random Seed:            999\n",
      "Alpha Range:            (0.01, 1.0)\n",
      "Discrete Alpha Steps:   10\n",
      "==================================================\n",
      "==================================================\n",
      "Predictor Configuration\n",
      "==================================================\n",
      "Window Length:          200\n",
      "Upsample K:             10\n",
      "Dataflow:               thumb_fr\n",
      "DB Parameter:           0.001\n",
      "Alpha:                  0.01\n",
      "Mode:                   fixed\n",
      "Direction:              forward\n",
      "Train Ratio:            0.6\n",
      "Train Data Augment:     False\n",
      "Smooth Fc:              1.5\n",
      "Smooth Order:           3\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "configIdx = 0\n",
    "envParams = getEnvConfig(configIdx)\n",
    "visualizeEnvConfig(envParams)\n",
    "predictorParams = getPredictorConfig(configIdx)\n",
    "visualizePredictorConfig(predictorParams)\n",
    "trafficDataParentPath = f'Results/TrafficData'\n",
    "simEnv = createEnv(envParams, trafficDataParentPath)\n",
    "simEnv.selectMode(mode=\"train\", type=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAC as Agent configconfig0's Training and Evaluation\n",
      "================================================================================\n",
      "Creating environment...\n",
      "Checking environment...\n",
      "Environment check passed!\n",
      "\n",
      "============================================================\n",
      "Training SAC as config0 Agent\n",
      "============================================================\n",
      "Total timesteps: 10000\n",
      "Environment: 4 users, 100 bandwidth\n",
      "Save path: Results/SAC_DrlAgent.zip\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Starting training...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -103     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 71       |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -55.8    |\n",
      "|    critic_loss     | 0.382    |\n",
      "|    ent_coef        | 0.406    |\n",
      "|    ent_coef_loss   | -15      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2999     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -97.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 55       |\n",
      "|    time_elapsed    | 143      |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65      |\n",
      "|    critic_loss     | 0.428    |\n",
      "|    ent_coef        | 0.124    |\n",
      "|    ent_coef_loss   | -30.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6999     |\n",
      "---------------------------------\n",
      "SAC training completed in 185.55 seconds\n",
      "Model saved to: Results/SAC_DrlAgent/config0.zip\n",
      "\n",
      "============================================================\n",
      "Evaluating SAC Agent\n",
      "============================================================\n",
      "Evaluation Results:\n",
      "  Average Reward: -0.0843 ± 0.0663\n",
      "  Average Loss Rate: 0.0947 ± 0.0078\n",
      "  Average Alpha: 0.0898\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "Algorithm: SAC\n",
      "Training completed in: 185.55 seconds\n",
      "Total training timesteps: 10000\n",
      "Average evaluation reward: -0.0843 ± 0.0663\n",
      "Average packet loss rate: 0.0947 ± 0.0078\n",
      "Average alpha value: 0.0898\n"
     ]
    }
   ],
   "source": [
    "# Configuration - change these parameters as needed\n",
    "ALGORITHM = \"SAC\"           # Options: \"SAC\", \"PPO\", \"A2C\", \"TD3\", \"DQN\"\n",
    "TIMESTEPS = 10000         # Training timesteps\n",
    "TIMESTEPS_PER_EPISODE = 1000\n",
    "SAVEPATH = f\"Results/{ALGORITHM}_DrlAgent\"\n",
    "AGENTNAME = f\"config{configIdx}\"\n",
    "OBVMODE = \"perfect\"\n",
    "# Run training and evaluation\n",
    "model, results = main(\n",
    "    envParams,\n",
    "    simEnv,\n",
    "    save_path=SAVEPATH,\n",
    "    agent_name=AGENTNAME,\n",
    "    algorithm_name=ALGORITHM, \n",
    "    total_timesteps=TIMESTEPS, \n",
    "    timesteps_per_episode=TIMESTEPS_PER_EPISODE,\n",
    "    obvMode=OBVMODE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stable_baselines3 import SAC\n",
    "#model = SAC.load(f\"{save_path}/{agentName}.zip\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic_predictor_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
