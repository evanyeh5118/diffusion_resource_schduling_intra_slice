{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "from DrlLibs.training import train_drl_agent\n",
    "from DrlLibs.evaluate import evaluate_drl_agent\n",
    "from DrlLibs.DRL_config import (\n",
    "    get_algorithm_config, \n",
    "    get_training_config,\n",
    "    print_algorithm_info\n",
    ")\n",
    "from DrlLibs import create_environment, check_env\n",
    "from Configs import getEnvConfig, visualizeEnvConfig, getPredictorConfig, visualizePredictorConfig\n",
    "from EnvLibs import PolicyDemoAdaptiveAlpha, PolicySimulator, createEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(simParams, simEnv, save_path, agent_name, algorithm_name: str = \"SAC\", total_timesteps: int = None):\n",
    "    \"\"\"Main function to train and evaluate a DRL agent.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"{algorithm_name} Agent Training and Evaluation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get training configuration\n",
    "    training_config = get_training_config()\n",
    "    if total_timesteps is None:\n",
    "        total_timesteps = training_config[\"total_timesteps\"]\n",
    "    \n",
    "    # Create environment\n",
    "    print(\"Creating environment...\")\n",
    "    env = create_environment(simParams, simEnv)\n",
    "    \n",
    "    # Check environment\n",
    "    print(\"Checking environment...\")\n",
    "    check_env(env.unwrapped)\n",
    "    print(\"Environment check passed!\")\n",
    "    \n",
    "    # Train DRL agent\n",
    "    model, callback, training_time = train_drl_agent(algorithm_name, env, total_timesteps, save_path, agent_name)\n",
    "    \n",
    "    # Evaluate DRL agent\n",
    "    eval_results = evaluate_drl_agent(model, env, algorithm_name)\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Algorithm: {algorithm_name}\")\n",
    "    print(f\"Training completed in: {training_time:.2f} seconds\")\n",
    "    print(f\"Total training timesteps: {total_timesteps}\")\n",
    "    print(f\"Average evaluation reward: {eval_results['avg_reward']:.4f} ± {eval_results['std_reward']:.4f}\")\n",
    "    print(f\"Average packet loss rate: {eval_results['avg_loss_rate']:.4f} ± {eval_results['std_loss_rate']:.4f}\")\n",
    "    print(f\"Average alpha value: {eval_results['avg_alpha']:.4f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return model, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Environment Configuration\n",
      "==================================================\n",
      "Number of Users:        4\n",
      "Window Length:          200\n",
      "Dataflow:               thumb_bk\n",
      "N_aggregation:          4\n",
      "Resource Bar:           4\n",
      "Bandwidth:              40\n",
      "M List:                 [2, 3]\n",
      "Random Seed:            999\n",
      "Alpha Range:            (0.01, 1.0)\n",
      "Discrete Alpha Steps:   10\n",
      "==================================================\n",
      "==================================================\n",
      "Predictor Configuration\n",
      "==================================================\n",
      "Window Length:          200\n",
      "Upsample K:             10\n",
      "Dataflow:               thumb_bk\n",
      "DB Parameter:           0.012\n",
      "Alpha:                  0.01\n",
      "Mode:                   fixed\n",
      "Direction:              backward\n",
      "Train Ratio:            0.6\n",
      "Train Data Augment:     False\n",
      "Smooth Fc:              1.5\n",
      "Smooth Order:           3\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "configIdx = 1\n",
    "envParams = getEnvConfig(configIdx)\n",
    "visualizeEnvConfig(envParams)\n",
    "predictorParams = getPredictorConfig(configIdx)\n",
    "visualizePredictorConfig(predictorParams)\n",
    "trafficDataParentPath = f'Results/TrafficData'\n",
    "simEnv = createEnv(envParams, trafficDataParentPath)\n",
    "simEnv.selectMode(mode=\"train\", type=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training with SAC with config1...\n",
      "================================================================================\n",
      "SAC Agent Training and Evaluation\n",
      "================================================================================\n",
      "Creating environment...\n",
      "Checking environment...\n",
      "Environment check passed!\n",
      "\n",
      "============================================================\n",
      "Training SAC Agent\n",
      "============================================================\n",
      "Total timesteps: 20000\n",
      "Environment: 4 users, 40 bandwidth\n",
      "Save path: Results/DrlAgent.zip\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Starting training...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -208     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 52       |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -54.7    |\n",
      "|    critic_loss     | 0.397    |\n",
      "|    ent_coef        | 0.406    |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2999     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -196     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 48       |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.8    |\n",
      "|    critic_loss     | 0.572    |\n",
      "|    ent_coef        | 0.124    |\n",
      "|    ent_coef_loss   | -33.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6999     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -173     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 257      |\n",
      "|    total_timesteps | 12000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -53.4    |\n",
      "|    critic_loss     | 0.27     |\n",
      "|    ent_coef        | 0.0456   |\n",
      "|    ent_coef_loss   | -31.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -163     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 346      |\n",
      "|    total_timesteps | 16000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -46.2    |\n",
      "|    critic_loss     | 0.153    |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -16.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14999    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Configuration - change these parameters as needed\n",
    "ALGORITHM = \"SAC\"           # Options: \"SAC\", \"PPO\", \"A2C\", \"TD3\", \"DQN\"\n",
    "TIMESTEPS = 20000         # Training timesteps\n",
    "SAVEPATH = f\"Results/DrlAgent\"\n",
    "AGENTNAME = f\"config{configIdx}\"\n",
    "# Run training and evaluation\n",
    "print(f\"\\nStarting training with {ALGORITHM} with config{configIdx}...\")\n",
    "model, results = main(\n",
    "    envParams,\n",
    "    simEnv,\n",
    "    save_path=SAVEPATH,\n",
    "    agent_name=AGENTNAME,\n",
    "    algorithm_name=ALGORITHM, \n",
    "    total_timesteps=TIMESTEPS, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stable_baselines3 import SAC\n",
    "#model = SAC.load(f\"{save_path}/{agentName}.zip\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic_predictor_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
