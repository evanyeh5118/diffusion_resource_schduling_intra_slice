{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "from DrlLibs.training import train_drl_agent\n",
    "from DrlLibs.evaluate import evaluate_drl_agent\n",
    "from DrlLibs import create_environment, check_env\n",
    "from Configs import getEnvConfig, visualizeEnvConfig, getPredictorConfig, visualizePredictorConfig\n",
    "from EnvLibs import PolicyDemoAdaptiveAlpha, PolicySimulator, createEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(simParams, simEnv, save_path, agent_name, algorithm_name: str = \"SAC\", \n",
    "         obvMode=\"perfect\",\n",
    "         total_timesteps: int = 20000, timesteps_per_episode: int = 5000):\n",
    "    \"\"\"Main function to train and evaluate a DRL agent.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"{algorithm_name} as Agent config{agent_name}'s Training and Evaluation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create environment\n",
    "    print(\"Creating environment...\")\n",
    "    env = create_environment(simParams, simEnv, obvMode, timesteps_per_episode)\n",
    "    \n",
    "    # Check environment\n",
    "    print(\"Checking environment...\")\n",
    "    check_env(env.unwrapped)\n",
    "    print(\"Environment check passed!\")\n",
    "    \n",
    "    # Train DRL agent\n",
    "    model, callback, training_time = train_drl_agent(algorithm_name, env, total_timesteps, save_path, agent_name)\n",
    "    \n",
    "    # Evaluate DRL agent\n",
    "    eval_results = evaluate_drl_agent(model, env, algorithm_name)\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Algorithm: {algorithm_name}\")\n",
    "    print(f\"Training completed in: {training_time:.2f} seconds\")\n",
    "    print(f\"Total training timesteps: {total_timesteps}\")\n",
    "    print(f\"Average evaluation reward: {eval_results['avg_reward']:.4f} ± {eval_results['std_reward']:.4f}\")\n",
    "    print(f\"Average packet loss rate: {eval_results['avg_loss_rate']:.4f} ± {eval_results['std_loss_rate']:.4f}\")\n",
    "    print(f\"Average alpha value: {eval_results['avg_alpha']:.4f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return model, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Environment Configuration\n",
      "==================================================\n",
      "Number of Users:        4\n",
      "Window Length:          200\n",
      "Dataflow:               thumb_fr\n",
      "N_aggregation:          4\n",
      "Resource Bar:           4\n",
      "Bandwidth:              1000\n",
      "M List:                 [2, 3]\n",
      "Random Seed:            999\n",
      "Alpha Range:            (0.01, 1.0)\n",
      "Discrete Alpha Steps:   10\n",
      "==================================================\n",
      "==================================================\n",
      "Predictor Configuration\n",
      "==================================================\n",
      "Window Length:          200\n",
      "Upsample K:             10\n",
      "Dataflow:               thumb_fr\n",
      "DB Parameter:           0.001\n",
      "Alpha:                  0.01\n",
      "Mode:                   fixed\n",
      "Direction:              forward\n",
      "Train Ratio:            0.6\n",
      "Train Data Augment:     False\n",
      "Smooth Fc:              1.5\n",
      "Smooth Order:           3\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "configIdx = 0\n",
    "envParams = getEnvConfig(configIdx)\n",
    "visualizeEnvConfig(envParams)\n",
    "predictorParams = getPredictorConfig(configIdx)\n",
    "visualizePredictorConfig(predictorParams)\n",
    "trafficDataParentPath = f'Results/TrafficData'\n",
    "simEnv = createEnv(envParams, trafficDataParentPath)\n",
    "simEnv.selectMode(mode=\"train\", type=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training with SAC with config0...\n",
      "================================================================================\n",
      "SAC Agent Training and Evaluation\n",
      "================================================================================\n",
      "Creating environment...\n",
      "Checking environment...\n",
      "Environment check passed!\n",
      "\n",
      "============================================================\n",
      "Training SAC as config0 Agent\n",
      "============================================================\n",
      "Total timesteps: 200000\n",
      "Environment: 4 users, 1000 bandwidth\n",
      "Save path: Results/DrlAgent.zip\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Starting training...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5e+03    |\n",
      "|    ep_rew_mean     | -601     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 29       |\n",
      "|    time_elapsed    | 675      |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -38.3    |\n",
      "|    critic_loss     | 0.0173   |\n",
      "|    ent_coef        | 0.00846  |\n",
      "|    ent_coef_loss   | -20.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5e+03    |\n",
      "|    ep_rew_mean     | -370     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 22       |\n",
      "|    time_elapsed    | 1771     |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.7    |\n",
      "|    critic_loss     | 0.00275  |\n",
      "|    ent_coef        | 0.00214  |\n",
      "|    ent_coef_loss   | -7.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 38999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5e+03    |\n",
      "|    ep_rew_mean     | -260     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 20       |\n",
      "|    time_elapsed    | 2908     |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.69    |\n",
      "|    critic_loss     | 0.0019   |\n",
      "|    ent_coef        | 0.000622 |\n",
      "|    ent_coef_loss   | 3.68     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 58999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5e+03    |\n",
      "|    ep_rew_mean     | -204     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 4110     |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.708   |\n",
      "|    critic_loss     | 0.0006   |\n",
      "|    ent_coef        | 0.000448 |\n",
      "|    ent_coef_loss   | -2.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 78999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5e+03    |\n",
      "|    ep_rew_mean     | -170     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 18       |\n",
      "|    time_elapsed    | 5332     |\n",
      "|    total_timesteps | 100000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.236    |\n",
      "|    critic_loss     | 0.000533 |\n",
      "|    ent_coef        | 0.000183 |\n",
      "|    ent_coef_loss   | -8.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 98999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5e+03    |\n",
      "|    ep_rew_mean     | -147     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 18       |\n",
      "|    time_elapsed    | 6336     |\n",
      "|    total_timesteps | 120000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.499    |\n",
      "|    critic_loss     | 8.1e-05  |\n",
      "|    ent_coef        | 9.64e-05 |\n",
      "|    ent_coef_loss   | 8.91     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 118999   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5e+03    |\n",
      "|    ep_rew_mean     | -130     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 20       |\n",
      "|    time_elapsed    | 6841     |\n",
      "|    total_timesteps | 140000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.587    |\n",
      "|    critic_loss     | 4.19e-05 |\n",
      "|    ent_coef        | 7.95e-05 |\n",
      "|    ent_coef_loss   | -5.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 138999   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5e+03    |\n",
      "|    ep_rew_mean     | -118     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 21       |\n",
      "|    time_elapsed    | 7428     |\n",
      "|    total_timesteps | 160000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.613    |\n",
      "|    critic_loss     | 2.98e-05 |\n",
      "|    ent_coef        | 6.24e-05 |\n",
      "|    ent_coef_loss   | -5.79    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 158999   |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Configuration - change these parameters as needed\n",
    "ALGORITHM = \"SAC\"           # Options: \"SAC\", \"PPO\", \"A2C\", \"TD3\", \"DQN\"\n",
    "TIMESTEPS = 200000         # Training timesteps\n",
    "TIMESTEPS_PER_EPISODE = 5000\n",
    "SAVEPATH = f\"Results/DrlAgent\"\n",
    "AGENTNAME = f\"config{configIdx}\"\n",
    "OBVMODE = \"perfect\"\n",
    "# Run training and evaluation\n",
    "model, results = main(\n",
    "    envParams,\n",
    "    simEnv,\n",
    "    save_path=SAVEPATH,\n",
    "    agent_name=AGENTNAME,\n",
    "    algorithm_name=ALGORITHM, \n",
    "    total_timesteps=TIMESTEPS, \n",
    "    timesteps_per_episode=TIMESTEPS_PER_EPISODE,\n",
    "    obvMode=OBVMODE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stable_baselines3 import SAC\n",
    "#model = SAC.load(f\"{save_path}/{agentName}.zip\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic_predictor_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
