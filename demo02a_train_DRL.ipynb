{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "from DrlLibs.training import train_drl_agent, create_parallel_environment\n",
    "from DrlLibs.evaluate import evaluate_drl_agent\n",
    "from DrlLibs import create_environment, check_env\n",
    "from Configs import getEnvConfig, visualizeEnvConfig, getPredictorConfig, visualizePredictorConfig\n",
    "from EnvLibs import PolicyDemoAdaptiveAlpha, PolicySimulator, createEnv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(simParams, simEnv, save_path, agent_name, algorithm_name: str = \"SAC\", \n",
    "         obvMode=\"perfect\", total_timesteps: int = 20000, \n",
    "         timesteps_per_episode: int = 5000, n_envs: int = 4):\n",
    "    \"\"\"Main function to train and evaluate a DRL agent.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"{algorithm_name} as Agent config{agent_name}'s Training and Evaluation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create environment (single or parallel)\n",
    "    print(\"Creating environment...\")\n",
    "    env = create_parallel_environment(simParams, simEnv, obvMode, \n",
    "                                    timesteps_per_episode, n_envs)\n",
    "    \n",
    "    # Check environment (only for single env)\n",
    "    if n_envs == 1:\n",
    "        print(\"Checking environment...\")\n",
    "        check_env(env.unwrapped)\n",
    "        print(\"Environment check passed!\")\n",
    "    else:\n",
    "        print(f\"Created {n_envs} parallel environments\")\n",
    "    \n",
    "    # Train DRL agent\n",
    "    model, callback, training_time = train_drl_agent(algorithm_name, env, total_timesteps, save_path, agent_name)\n",
    "    \n",
    "    # Create a clean single environment for evaluation\n",
    "    print(\"Creating evaluation environment...\")\n",
    "    eval_env = create_environment(simParams, simEnv, obvMode, timesteps_per_episode)\n",
    "    \n",
    "    # Evaluate DRL agent\n",
    "    eval_results = evaluate_drl_agent(model, eval_env, algorithm_name)\n",
    "    \n",
    "    # Close evaluation environment\n",
    "    eval_env.close()\n",
    "    \n",
    "    # Plot training results\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    try:\n",
    "        from DrlLibs.visualize import plot_training_results\n",
    "        plot_training_results(callback, eval_results, algorithm_name, save_plots=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Visualization failed: {e}\")\n",
    "        print(\"Training data summary:\")\n",
    "        if callback.episode_rewards:\n",
    "            print(f\"  Episodes completed: {len(callback.episode_rewards)}\")\n",
    "            print(f\"  Reward progression: {callback.episode_rewards[-10:]}\")  # Last 10 rewards\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Algorithm: {algorithm_name}\")\n",
    "    print(f\"Training completed in: {training_time:.2f} seconds\")\n",
    "    print(f\"Total training timesteps: {total_timesteps}\")\n",
    "    \n",
    "    if callback.episode_rewards:\n",
    "        print(f\"Total episodes completed: {len(callback.episode_rewards)}\")\n",
    "        print(f\"Average episode reward: {np.mean(callback.episode_rewards):.4f} ± {np.std(callback.episode_rewards):.4f}\")\n",
    "        print(f\"Final episode reward: {callback.episode_rewards[-1]:.4f}\")\n",
    "        print(f\"Best episode reward: {max(callback.episode_rewards):.4f}\")\n",
    "        print(f\"Reward improvement: {callback.episode_rewards[-1] - callback.episode_rewards[0]:.4f}\")\n",
    "    \n",
    "    print(f\"Average evaluation reward: {eval_results['avg_reward']:.4f} ± {eval_results['std_reward']:.4f}\")\n",
    "    print(f\"Average packet loss rate: {eval_results['avg_loss_rate']:.4f} ± {eval_results['std_loss_rate']:.4f}\")\n",
    "    print(f\"Average alpha value: {eval_results['avg_alpha']:.4f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return model, eval_results, callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Environment Configuration\n",
      "==================================================\n",
      "Number of Users:        8\n",
      "Window Length:          200\n",
      "Dataflow:               thumb_bk\n",
      "N_aggregation:          4\n",
      "Resource Bar:           5\n",
      "Bandwidth:              200\n",
      "M List:                 [3, 4, 5]\n",
      "Random Seed:            999\n",
      "Alpha Range:            (0.01, 1.0)\n",
      "Discrete Alpha Steps:   10\n",
      "==================================================\n",
      "==================================================\n",
      "Predictor Configuration\n",
      "==================================================\n",
      "Window Length:          200\n",
      "Upsample K:             10\n",
      "Dataflow:               thumb_fr\n",
      "DB Parameter:           0.012\n",
      "Alpha:                  0.01\n",
      "Mode:                   fixed\n",
      "Direction:              forward\n",
      "Train Ratio:            0.6\n",
      "Train Data Augment:     False\n",
      "Smooth Fc:              1.5\n",
      "Smooth Order:           3\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "configIdx = 5\n",
    "envParams = getEnvConfig(configIdx)\n",
    "visualizeEnvConfig(envParams)\n",
    "predictorParams = getPredictorConfig(configIdx)\n",
    "visualizePredictorConfig(predictorParams)\n",
    "trafficDataParentPath = f'Results/TrafficData'\n",
    "simEnv = createEnv(envParams, trafficDataParentPath)\n",
    "simEnv.selectMode(mode=\"train\", type=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "A2C as Agent configconfig5's Training and Evaluation\n",
      "================================================================================\n",
      "Creating environment...\n",
      "Checking environment...\n",
      "Environment check passed!\n",
      "\n",
      "============================================================\n",
      "Training A2C as config5 Agent\n",
      "============================================================\n",
      "Total timesteps: 100000\n",
      "Environment: 8 users, 200 bandwidth\n",
      "Save path: Results/DrlAgent/A2C.zip\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shd-haplab2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 256      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -25.8    |\n",
      "|    explained_variance | -0.0114  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -30.3    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.68     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -265     |\n",
      "| time/                 |          |\n",
      "|    fps                | 267      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -25.8    |\n",
      "|    explained_variance | -0.649   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -4.41    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.0675   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -265     |\n",
      "| time/                 |          |\n",
      "|    fps                | 272      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -25.9    |\n",
      "|    explained_variance | -5.76    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -1.21    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.0283   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -219     |\n",
      "| time/                 |          |\n",
      "|    fps                | 276      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -25.9    |\n",
      "|    explained_variance | -0.0692  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 6.94     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.0805   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -219     |\n",
      "| time/                 |          |\n",
      "|    fps                | 271      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -25.9    |\n",
      "|    explained_variance | -0.503   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 17.8     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.498    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -167     |\n",
      "| time/                 |          |\n",
      "|    fps                | 261      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -25.9    |\n",
      "|    explained_variance | -1.59    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 4.59     |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.0475   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -167     |\n",
      "| time/                 |          |\n",
      "|    fps                | 249      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -26      |\n",
      "|    explained_variance | -3.72    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 3.87     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 0.0279   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -132     |\n",
      "| time/                 |          |\n",
      "|    fps                | 240      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -26.4    |\n",
      "|    explained_variance | -0.0138  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -13.1    |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 0.395    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -132     |\n",
      "| time/                 |          |\n",
      "|    fps                | 234      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -26.8    |\n",
      "|    explained_variance | -0.851   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.0298   |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 0.000184 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -108     |\n",
      "| time/                 |          |\n",
      "|    fps                | 230      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -27.4    |\n",
      "|    explained_variance | 0.425    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 1.25     |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 0.0019   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -108     |\n",
      "| time/                 |          |\n",
      "|    fps                | 226      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -28.1    |\n",
      "|    explained_variance | -4.57    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.135   |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 0.000194 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -91.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 223      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -28.8    |\n",
      "|    explained_variance | -4.04    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.0489   |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 9.78e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -91.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 220      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -29.5    |\n",
      "|    explained_variance | -23.2    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.267    |\n",
      "|    std                | 1.25     |\n",
      "|    value_loss         | 0.000218 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -79.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 218      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -30.2    |\n",
      "|    explained_variance | -5.94    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.854    |\n",
      "|    std                | 1.29     |\n",
      "|    value_loss         | 0.00086  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -79.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 217      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -30.9    |\n",
      "|    explained_variance | 0.47     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.496   |\n",
      "|    std                | 1.35     |\n",
      "|    value_loss         | 0.00029  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -70.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 215      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -31.6    |\n",
      "|    explained_variance | -5.16    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.548    |\n",
      "|    std                | 1.4      |\n",
      "|    value_loss         | 0.000373 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -70.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 214      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -32.3    |\n",
      "|    explained_variance | 0.609    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.335    |\n",
      "|    std                | 1.46     |\n",
      "|    value_loss         | 0.00013  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -64.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 213      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 42       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -33      |\n",
      "|    explained_variance | -18.1    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.0262  |\n",
      "|    std                | 1.51     |\n",
      "|    value_loss         | 0.000131 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -64.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 212      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -33.6    |\n",
      "|    explained_variance | -11.6    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.0155   |\n",
      "|    std                | 1.57     |\n",
      "|    value_loss         | 5.89e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -59      |\n",
      "| time/                 |          |\n",
      "|    fps                | 211      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 47       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -34.3    |\n",
      "|    explained_variance | -0.304   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.0573  |\n",
      "|    std                | 1.63     |\n",
      "|    value_loss         | 1.71e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -59      |\n",
      "| time/                 |          |\n",
      "|    fps                | 210      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -34.9    |\n",
      "|    explained_variance | -5.53    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 0.328    |\n",
      "|    std                | 1.68     |\n",
      "|    value_loss         | 0.000131 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -54.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 210      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -35.4    |\n",
      "|    explained_variance | -0.273   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 0.186    |\n",
      "|    std                | 1.74     |\n",
      "|    value_loss         | 4.41e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -54.6    |\n",
      "| time/                 |          |\n",
      "|    fps                | 210      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -36.1    |\n",
      "|    explained_variance | -6.97    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -0.0242  |\n",
      "|    std                | 1.8      |\n",
      "|    value_loss         | 4.5e-05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -51      |\n",
      "| time/                 |          |\n",
      "|    fps                | 210      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 56       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -36.6    |\n",
      "|    explained_variance | -1.98    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | -0.453   |\n",
      "|    std                | 1.86     |\n",
      "|    value_loss         | 0.000656 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -51      |\n",
      "| time/                 |          |\n",
      "|    fps                | 212      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 58       |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -37.3    |\n",
      "|    explained_variance | -0.481   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | -0.975   |\n",
      "|    std                | 1.93     |\n",
      "|    value_loss         | 0.000797 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -47.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 212      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -38      |\n",
      "|    explained_variance | -4.82    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 0.0515   |\n",
      "|    std                | 2        |\n",
      "|    value_loss         | 6.93e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -47.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 212      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -38.6    |\n",
      "|    explained_variance | -4.19    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0.223   |\n",
      "|    std                | 2.07     |\n",
      "|    value_loss         | 0.000112 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -45.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 211      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 66       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -39.2    |\n",
      "|    explained_variance | -0.611   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 0.477    |\n",
      "|    std                | 2.14     |\n",
      "|    value_loss         | 0.000212 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1e+03    |\n",
      "|    ep_rew_mean        | -45.2    |\n",
      "| time/                 |          |\n",
      "|    fps                | 211      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 68       |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -39.8    |\n",
      "|    explained_variance | -6.56    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 0.239    |\n",
      "|    std                | 2.21     |\n",
      "|    value_loss         | 0.00016  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m OBVMODE = \u001b[33m\"\u001b[39m\u001b[33mperfect\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Run training and evaluation\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Run training and evaluation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m model, results, callback = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43menvParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43msimEnv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAVEPATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAGENTNAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43malgorithm_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mALGORITHM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimesteps_per_episode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTIMESTEPS_PER_EPISODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobvMode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOBVMODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_ENVS\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(simParams, simEnv, save_path, agent_name, algorithm_name, obvMode, total_timesteps, timesteps_per_episode, n_envs)\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_envs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m parallel environments\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Train DRL agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m model, callback, training_time = \u001b[43mtrain_drl_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Create a clean single environment for evaluation\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating evaluation environment...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shd-haplab2\\OneDrive - Aarhus universitet\\PhD-Gkokkinis\\diffusion_resource_schduling_intra_slice\\DrlLibs\\training.py:127\u001b[39m, in \u001b[36mtrain_drl_agent\u001b[39m\u001b[34m(algorithm_name, env, total_timesteps, save_path, agentName)\u001b[39m\n\u001b[32m    125\u001b[39m start_time = time.time()\n\u001b[32m    126\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m training_time = time.time() - start_time\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shd-haplab2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:201\u001b[39m, in \u001b[36mA2C.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    193\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfA2C,\n\u001b[32m    194\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    199\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    200\u001b[39m ) -> SelfA2C:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shd-haplab2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shd-haplab2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:247\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    244\u001b[39m             terminal_value = \u001b[38;5;28mself\u001b[39m.policy.predict_values(terminal_obs)[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    245\u001b[39m         rewards[idx] += \u001b[38;5;28mself\u001b[39m.gamma * terminal_value\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[43mrollout_buffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_last_episode_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._last_obs = new_obs  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._last_episode_starts = dones\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shd-haplab2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:475\u001b[39m, in \u001b[36mRolloutBuffer.add\u001b[39m\u001b[34m(self, obs, action, reward, episode_start, value, log_prob)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28mself\u001b[39m.rewards[\u001b[38;5;28mself\u001b[39m.pos] = np.array(reward)\n\u001b[32m    474\u001b[39m \u001b[38;5;28mself\u001b[39m.episode_starts[\u001b[38;5;28mself\u001b[39m.pos] = np.array(episode_start)\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m \u001b[38;5;28mself\u001b[39m.values[\u001b[38;5;28mself\u001b[39m.pos] = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy().flatten()\n\u001b[32m    476\u001b[39m \u001b[38;5;28mself\u001b[39m.log_probs[\u001b[38;5;28mself\u001b[39m.pos] = log_prob.clone().cpu().numpy()\n\u001b[32m    477\u001b[39m \u001b[38;5;28mself\u001b[39m.pos += \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Configuration - change these parameters as needed\n",
    "ALGORITHM = \"A2C\"           # Options: \"SAC\", \"PPO\", \"A2C\", \"TD3\", \"DQN\"\n",
    "TIMESTEPS = 100000         # Training timesteps\n",
    "TIMESTEPS_PER_EPISODE = 1000\n",
    "N_ENVS = 1\n",
    "SAVEPATH = f\"Results/DrlAgent/{ALGORITHM}\"\n",
    "AGENTNAME = f\"config{configIdx}\"\n",
    "OBVMODE = \"perfect\"\n",
    "# Run training and evaluation\n",
    "# Run training and evaluation\n",
    "model, results, callback = main(\n",
    "    envParams,\n",
    "    simEnv,\n",
    "    save_path=SAVEPATH,\n",
    "    agent_name=AGENTNAME,\n",
    "    algorithm_name=ALGORITHM, \n",
    "    total_timesteps=TIMESTEPS, \n",
    "    timesteps_per_episode=TIMESTEPS_PER_EPISODE,\n",
    "    obvMode=OBVMODE,\n",
    "    n_envs=N_ENVS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stable_baselines3 import SAC\n",
    "#model = SAC.load(f\"{save_path}/{agentName}.zip\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
