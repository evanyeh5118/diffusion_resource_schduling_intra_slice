{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "from DrlLibs.training import train_drl_agent\n",
    "from DrlLibs.evaluate import evaluate_drl_agent\n",
    "from DrlLibs import create_environment, check_env\n",
    "from Configs import getEnvConfig, visualizeEnvConfig, getPredictorConfig, visualizePredictorConfig\n",
    "from EnvLibs import PolicyDemoAdaptiveAlpha, PolicySimulator, createEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(simParams, simEnv, save_path, agent_name, algorithm_name: str = \"SAC\", \n",
    "         obvMode=\"perfect\",\n",
    "         total_timesteps: int = 20000, timesteps_per_episode: int = 5000):\n",
    "    \"\"\"Main function to train and evaluate a DRL agent.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"{algorithm_name} as Agent config{agent_name}'s Training and Evaluation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create environment\n",
    "    print(\"Creating environment...\")\n",
    "    env = create_environment(simParams, simEnv, obvMode, timesteps_per_episode)\n",
    "    \n",
    "    # Check environment\n",
    "    print(\"Checking environment...\")\n",
    "    check_env(env.unwrapped)\n",
    "    print(\"Environment check passed!\")\n",
    "    \n",
    "    # Train DRL agent\n",
    "    model, callback, training_time = train_drl_agent(algorithm_name, env, total_timesteps, save_path, agent_name)\n",
    "    \n",
    "    # Evaluate DRL agent\n",
    "    eval_results = evaluate_drl_agent(model, env, algorithm_name)\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Algorithm: {algorithm_name}\")\n",
    "    print(f\"Training completed in: {training_time:.2f} seconds\")\n",
    "    print(f\"Total training timesteps: {total_timesteps}\")\n",
    "    print(f\"Average evaluation reward: {eval_results['avg_reward']:.4f} ± {eval_results['std_reward']:.4f}\")\n",
    "    print(f\"Average packet loss rate: {eval_results['avg_loss_rate']:.4f} ± {eval_results['std_loss_rate']:.4f}\")\n",
    "    print(f\"Average alpha value: {eval_results['avg_alpha']:.4f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return model, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Environment Configuration\n",
      "==================================================\n",
      "Number of Users:        4\n",
      "Window Length:          200\n",
      "Dataflow:               thumb_fr\n",
      "N_aggregation:          4\n",
      "Resource Bar:           5\n",
      "Bandwidth:              100\n",
      "M List:                 [3, 4, 5]\n",
      "Random Seed:            999\n",
      "Alpha Range:            (0.01, 1.0)\n",
      "Discrete Alpha Steps:   10\n",
      "==================================================\n",
      "==================================================\n",
      "Predictor Configuration\n",
      "==================================================\n",
      "Window Length:          200\n",
      "Upsample K:             10\n",
      "Dataflow:               thumb_fr\n",
      "DB Parameter:           0.001\n",
      "Alpha:                  0.01\n",
      "Mode:                   fixed\n",
      "Direction:              forward\n",
      "Train Ratio:            0.6\n",
      "Train Data Augment:     False\n",
      "Smooth Fc:              1.5\n",
      "Smooth Order:           3\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "configIdx = 0\n",
    "envParams = getEnvConfig(configIdx)\n",
    "visualizeEnvConfig(envParams)\n",
    "predictorParams = getPredictorConfig(configIdx)\n",
    "visualizePredictorConfig(predictorParams)\n",
    "trafficDataParentPath = f'Results/TrafficData'\n",
    "simEnv = createEnv(envParams, trafficDataParentPath)\n",
    "simEnv.selectMode(mode=\"train\", type=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAC as Agent configconfig0's Training and Evaluation\n",
      "================================================================================\n",
      "Creating environment...\n",
      "Checking environment...\n",
      "Environment check passed!\n",
      "\n",
      "============================================================\n",
      "Training SAC as config0 Agent\n",
      "============================================================\n",
      "Total timesteps: 200000\n",
      "Environment: 4 users, 100 bandwidth\n",
      "Save path: Results/DrlAgent/SAC.zip\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Starting training...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -97.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 60       |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -56.2    |\n",
      "|    critic_loss     | 0.386    |\n",
      "|    ent_coef        | 0.406    |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2999     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -91      |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 45       |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.5    |\n",
      "|    critic_loss     | 4.2      |\n",
      "|    ent_coef        | 0.123    |\n",
      "|    ent_coef_loss   | -32      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6999     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -87.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 42       |\n",
      "|    time_elapsed    | 285      |\n",
      "|    total_timesteps | 12000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -59      |\n",
      "|    critic_loss     | 0.266    |\n",
      "|    ent_coef        | 0.0507   |\n",
      "|    ent_coef_loss   | -26.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -88.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 412      |\n",
      "|    total_timesteps | 16000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -54.2    |\n",
      "|    critic_loss     | 0.184    |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 10.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -85.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 521      |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -46.3    |\n",
      "|    critic_loss     | 7.94     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | 4.83     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 646      |\n",
      "|    total_timesteps | 24000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -39.7    |\n",
      "|    critic_loss     | 0.164    |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | 3.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -87.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 787      |\n",
      "|    total_timesteps | 28000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -34.2    |\n",
      "|    critic_loss     | 3.61     |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | -6.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -82.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 935      |\n",
      "|    total_timesteps | 32000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -29.3    |\n",
      "|    critic_loss     | 0.0462   |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | 4.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 30999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -80.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 1082     |\n",
      "|    total_timesteps | 36000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -25.8    |\n",
      "|    critic_loss     | 5.13     |\n",
      "|    ent_coef        | 0.0178   |\n",
      "|    ent_coef_loss   | 8.46     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 34999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -77.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 1226     |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.3    |\n",
      "|    critic_loss     | 0.0702   |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | 15.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 38999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -74.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 1371     |\n",
      "|    total_timesteps | 44000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.2    |\n",
      "|    critic_loss     | 0.0289   |\n",
      "|    ent_coef        | 0.00752  |\n",
      "|    ent_coef_loss   | -15.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 42999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -71.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 31       |\n",
      "|    time_elapsed    | 1511     |\n",
      "|    total_timesteps | 48000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.7    |\n",
      "|    critic_loss     | 0.899    |\n",
      "|    ent_coef        | 0.00927  |\n",
      "|    ent_coef_loss   | -2.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 46999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -69.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 31       |\n",
      "|    time_elapsed    | 1659     |\n",
      "|    total_timesteps | 52000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.9    |\n",
      "|    critic_loss     | 0.0146   |\n",
      "|    ent_coef        | 0.00706  |\n",
      "|    ent_coef_loss   | -4.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 50999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -65.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 30       |\n",
      "|    time_elapsed    | 1808     |\n",
      "|    total_timesteps | 56000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.5    |\n",
      "|    critic_loss     | 0.0156   |\n",
      "|    ent_coef        | 0.00668  |\n",
      "|    ent_coef_loss   | -3.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 54999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -62.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 30       |\n",
      "|    time_elapsed    | 1957     |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.66    |\n",
      "|    critic_loss     | 0.21     |\n",
      "|    ent_coef        | 0.00524  |\n",
      "|    ent_coef_loss   | -2.54    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 58999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -59.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 30       |\n",
      "|    time_elapsed    | 2104     |\n",
      "|    total_timesteps | 64000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.67    |\n",
      "|    critic_loss     | 0.00444  |\n",
      "|    ent_coef        | 0.00453  |\n",
      "|    ent_coef_loss   | -5.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 62999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -56.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 30       |\n",
      "|    time_elapsed    | 2254     |\n",
      "|    total_timesteps | 68000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.29    |\n",
      "|    critic_loss     | 0.105    |\n",
      "|    ent_coef        | 0.00285  |\n",
      "|    ent_coef_loss   | 13.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 66999    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m OBVMODE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperfect\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Run training and evaluation\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model, results \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimEnv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAVEPATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAGENTNAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgorithm_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALGORITHM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimesteps_per_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIMESTEPS_PER_EPISODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobvMode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOBVMODE\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(simParams, simEnv, save_path, agent_name, algorithm_name, obvMode, total_timesteps, timesteps_per_episode)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment check passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Train DRL agent\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model, callback, training_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_drl_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Evaluate DRL agent\u001b[39;00m\n\u001b[0;32m     23\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m evaluate_drl_agent(model, env, algorithm_name)\n",
      "File \u001b[1;32mc:\\Users\\Ye\\Documents\\YuYeh_Documents\\L2S\\Projects\\diffusion_resource_schduling_intra_slice\\DrlLibs\\training.py:85\u001b[0m, in \u001b[0;36mtrain_drl_agent\u001b[1;34m(algorithm_name, env, total_timesteps, save_path, agentName)\u001b[0m\n\u001b[0;32m     83\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ye\\miniconda3\\envs\\traffic_predictor_3_9\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:308\u001b[0m, in \u001b[0;36mSAC.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[0;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ye\\miniconda3\\envs\\traffic_predictor_3_9\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ye\\miniconda3\\envs\\traffic_predictor_3_9\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:281\u001b[0m, in \u001b[0;36mSAC.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# Optimize the actor\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 281\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Update target networks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ye\\miniconda3\\envs\\traffic_predictor_3_9\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ye\\miniconda3\\envs\\traffic_predictor_3_9\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ye\\miniconda3\\envs\\traffic_predictor_3_9\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configuration - change these parameters as needed\n",
    "ALGORITHM = \"SAC\"           # Options: \"SAC\", \"PPO\", \"A2C\", \"TD3\", \"DQN\"\n",
    "TIMESTEPS = 200000         # Training timesteps\n",
    "TIMESTEPS_PER_EPISODE = 1000\n",
    "SAVEPATH = f\"Results/DrlAgent/{ALGORITHM}\"\n",
    "AGENTNAME = f\"config{configIdx}\"\n",
    "OBVMODE = \"perfect\"\n",
    "# Run training and evaluation\n",
    "model, results = main(\n",
    "    envParams,\n",
    "    simEnv,\n",
    "    save_path=SAVEPATH,\n",
    "    agent_name=AGENTNAME,\n",
    "    algorithm_name=ALGORITHM, \n",
    "    total_timesteps=TIMESTEPS, \n",
    "    timesteps_per_episode=TIMESTEPS_PER_EPISODE,\n",
    "    obvMode=OBVMODE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stable_baselines3 import SAC\n",
    "#model = SAC.load(f\"{save_path}/{agentName}.zip\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic_predictor_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
