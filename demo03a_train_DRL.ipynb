{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "from DrlLibs.training import train_drl_agent\n",
    "from DrlLibs.evaluate import evaluate_drl_agent\n",
    "from DrlLibs.DRL_config import (\n",
    "    get_algorithm_config, \n",
    "    get_training_config,\n",
    "    print_algorithm_info\n",
    ")\n",
    "from DrlLibs import create_environment, check_env\n",
    "from EnvLibs import getEnvConfig, createEnv, visualizeEnvConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(simParams, simEnv, save_path, agent_name, algorithm_name: str = \"SAC\", total_timesteps: int = None):\n",
    "    \"\"\"Main function to train and evaluate a DRL agent.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"{algorithm_name} Agent Training and Evaluation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get training configuration\n",
    "    training_config = get_training_config()\n",
    "    if total_timesteps is None:\n",
    "        total_timesteps = training_config[\"total_timesteps\"]\n",
    "    \n",
    "    # Create environment\n",
    "    print(\"Creating environment...\")\n",
    "    env = create_environment(simParams, simEnv)\n",
    "    \n",
    "    # Check environment\n",
    "    print(\"Checking environment...\")\n",
    "    check_env(env.unwrapped)\n",
    "    print(\"Environment check passed!\")\n",
    "    \n",
    "    # Train DRL agent\n",
    "    model, callback, training_time = train_drl_agent(algorithm_name, env, total_timesteps, save_path, agent_name)\n",
    "    \n",
    "    # Evaluate DRL agent\n",
    "    eval_results = evaluate_drl_agent(model, env, algorithm_name)\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Algorithm: {algorithm_name}\")\n",
    "    print(f\"Training completed in: {training_time:.2f} seconds\")\n",
    "    print(f\"Total training timesteps: {total_timesteps}\")\n",
    "    print(f\"Average evaluation reward: {eval_results['avg_reward']:.4f} ± {eval_results['std_reward']:.4f}\")\n",
    "    print(f\"Average packet loss rate: {eval_results['avg_loss_rate']:.4f} ± {eval_results['std_loss_rate']:.4f}\")\n",
    "    print(f\"Average alpha value: {eval_results['avg_alpha']:.4f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return model, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Environment Configuration\n",
      "==================================================\n",
      "Number of Users:        4\n",
      "Window Length:          10\n",
      "Resource Bar:           4\n",
      "Bandwidth:              40\n",
      "M List:                 [2, 3]\n",
      "Random Seed:            999\n",
      "Alpha Range:            (0.01, 1.0)\n",
      "Discrete Alpha Steps:   10\n",
      "==================================================\n",
      "\n",
      "Starting training with SAC...\n",
      "================================================================================\n",
      "SAC Agent Training and Evaluation\n",
      "================================================================================\n",
      "Creating environment...\n",
      "Checking environment...\n",
      "Environment check passed!\n",
      "\n",
      "============================================================\n",
      "Training SAC Agent\n",
      "============================================================\n",
      "Total timesteps: 20000\n",
      "Environment: 4 users, 40 bandwidth\n",
      "Save path: Results/DrlAgent.zip\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Starting training...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -172     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 63       |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -54.7    |\n",
      "|    critic_loss     | 0.383    |\n",
      "|    ent_coef        | 0.406    |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2999     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -162     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -62.3    |\n",
      "|    critic_loss     | 4.45     |\n",
      "|    ent_coef        | 0.124    |\n",
      "|    ent_coef_loss   | -33      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6999     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -148     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 49       |\n",
      "|    time_elapsed    | 243      |\n",
      "|    total_timesteps | 12000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -54.5    |\n",
      "|    critic_loss     | 0.337    |\n",
      "|    ent_coef        | 0.0461   |\n",
      "|    ent_coef_loss   | -24.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -140     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 48       |\n",
      "|    time_elapsed    | 332      |\n",
      "|    total_timesteps | 16000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -50.8    |\n",
      "|    critic_loss     | 8.25     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 7.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -133     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 425      |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -43.5    |\n",
      "|    critic_loss     | 0.17     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | 4.72     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18999    |\n",
      "---------------------------------\n",
      "SAC training completed in 425.61 seconds\n",
      "Model saved to: Results/DrlAgent/SAC_NumUser4_B40_LenWindow10_thumb_fr.zip\n",
      "\n",
      "============================================================\n",
      "Evaluating SAC Agent\n",
      "============================================================\n",
      "Evaluation Results:\n",
      "  Average Reward: -35.1787 ± 4.6575\n",
      "  Average Loss Rate: 0.0394 ± 0.0054\n",
      "  Average Alpha: 0.6692\n",
      "  Average Episode Length: 1000.00\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "Algorithm: SAC\n",
      "Training completed in: 425.61 seconds\n",
      "Total training timesteps: 20000\n",
      "Average evaluation reward: -35.1787 ± 4.6575\n",
      "Average packet loss rate: 0.0394 ± 0.0054\n",
      "Average alpha value: 0.6692\n"
     ]
    }
   ],
   "source": [
    "configIdx = 0\n",
    "simParams = getEnvConfig(configIdx)\n",
    "simEnv = createEnv(simParams)\n",
    "visualizeEnvConfig(simParams)\n",
    "\n",
    "# Configuration - change these parameters as needed\n",
    "ALGORITHM = \"SAC\"           # Options: \"SAC\", \"PPO\", \"A2C\", \"TD3\", \"DQN\"\n",
    "TIMESTEPS = 20000         # Training timesteps\n",
    "SAVEPATH = f\"Results/DrlAgent\"\n",
    "AGENTNAME = f\"config{configIdx}\"\n",
    "# Run training and evaluation\n",
    "print(f\"\\nStarting training with {ALGORITHM} with config{configIdx}...\")\n",
    "model, results = main(\n",
    "    simParams,\n",
    "    simEnv,\n",
    "    save_path=SAVEPATH,\n",
    "    agent_name=AGENTNAME,\n",
    "    algorithm_name=ALGORITHM, \n",
    "    total_timesteps=TIMESTEPS, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stable_baselines3 import SAC\n",
    "#model = SAC.load(f\"{save_path}/{agentName}.zip\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic_predictor_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
